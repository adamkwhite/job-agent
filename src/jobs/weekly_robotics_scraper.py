"""
Weekly Robotics/Deeptech Job Scraper
Run weekly to check for new high-scoring robotics jobs
Can be scheduled via cron or run manually
"""

import json
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from agents.job_scorer import JobScorer
from database import JobDatabase
from job_filter import JobFilter
from models import OpportunityData
from notifier import JobNotifier
from scrapers.robotics_deeptech_scraper import RoboticsDeeptechScraper


class WeeklyRoboticsJobChecker:
    """Weekly check for new robotics/deeptech opportunities"""

    def __init__(self, profile: str | None = None):
        self.scraper = RoboticsDeeptechScraper()
        self.scorer = JobScorer()
        self.filter = JobFilter()
        self.database = JobDatabase(profile=profile)
        self.notifier = JobNotifier()
        self.profile = profile

    def _create_scraping_error_issue(
        self, company: str, url: str, error_msg: str, failure_count: int
    ) -> str | None:
        """
        Create a GitHub issue for repeated Firecrawl scraping failures.

        Args:
            company: Company name
            url: Career page URL
            error_msg: Error message details
            failure_count: Number of consecutive failures

        Returns:
            Issue URL if created, None if creation failed
        """
        try:
            issue_title = f"[Firecrawl] Repeated scraping failures for {company}"
            issue_body = f"""## Problem
Firecrawl scraping has failed {failure_count} consecutive times for {company}.

**Company**: {company}
**URL**: {url}
**Latest Error**: {error_msg}
**Failure Count**: {failure_count}
**Last Attempt**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Investigation Steps
1. Verify URL is still valid and accessible
2. Check if career page structure changed
3. Test Firecrawl command manually
4. Review markdown output for parsing issues
5. Consider updating extraction patterns

## Related Files
- `src/jobs/weekly_robotics_scraper.py:process_firecrawl_markdown()`
- `config/robotics_priority_companies.json`
- Markdown cache: `data/firecrawl_cache/{company.lower().replace(" ", "_")}_*.md`

## Acceptance Criteria
- [ ] Root cause identified
- [ ] Fix implemented (or company removed from priority list)
- [ ] Scraping working for {company}
- [ ] Tests updated if extraction patterns changed

ðŸ¤– Auto-generated by weekly robotics scraper

Related: #65 (Firecrawl generic career pages)
"""

            result = subprocess.run(
                [
                    "gh",
                    "issue",
                    "create",
                    "--title",
                    issue_title,
                    "--body",
                    issue_body,
                    "--label",
                    "firecrawl-scraping,robotics-sheet,bug",
                ],
                capture_output=True,
                text=True,
                check=True,
                timeout=30,
            )

            # Extract issue URL from output
            issue_url = result.stdout.strip().split("\n")[-1]
            print(f"  âœ“ Created GitHub issue: {issue_url}")
            return issue_url

        except subprocess.CalledProcessError as e:
            print(f"  âœ— Failed to create GitHub issue: {e.stderr}")
            return None
        except subprocess.TimeoutExpired:
            print("  âœ— GitHub issue creation timed out")
            return None
        except Exception as e:
            print(f"  âœ— Unexpected error creating GitHub issue: {e}")
            return None

    def process_firecrawl_markdown(self, markdown_path: str, company: str) -> list[OpportunityData]:
        """
        Process Firecrawl markdown output and extract job listings.

        Args:
            markdown_path: Path to saved Firecrawl markdown file
            company: Company name for attribution

        Returns:
            List of OpportunityData extracted from markdown
        """
        jobs = []

        try:
            with open(markdown_path) as f:
                content = f.read()

            # Leadership keywords for filtering
            leadership_keywords = [
                "director",
                "vp",
                "vice president",
                "head of",
                "chief",
                "executive",
                "principal",
                "staff",
                "senior manager",
                "lead",
            ]

            # Pattern 1: Look for job titles in headings
            # Match patterns like "## Director of Engineering" or "### VP of Product"
            heading_pattern = r"^#+\s+(.+)$"
            headings = re.findall(heading_pattern, content, re.MULTILINE)

            # Pattern 2: URL pattern for job links
            # Match various ATS patterns
            url_pattern = r"https?://[^\s)]+(?:job|career|position)[^\s)]*"

            # Extract jobs from headings that look like job titles
            for heading in headings:
                # Check if heading looks like a job title (contains leadership keyword)
                if any(kw in heading.lower() for kw in leadership_keywords):
                    # Try to find associated URL nearby in content
                    # This is a simple heuristic - look for URLs within 500 chars after heading
                    heading_pos = content.find(heading)
                    if heading_pos != -1:
                        nearby_content = content[heading_pos : heading_pos + 500]
                        nearby_urls = re.findall(url_pattern, nearby_content, re.IGNORECASE)
                        job_url = (
                            nearby_urls[0]
                            if nearby_urls
                            else f"https://{company.lower().replace(' ', '')}.com/careers"
                        )

                        job = OpportunityData(
                            source="robotics_deeptech_sheet",
                            source_email="",
                            type="direct_job",
                            company=company,
                            title=heading.strip(),
                            location="",
                            link=job_url,
                            description=f"Discovered via Firecrawl scraping of {company} career page",
                            job_type="",
                            needs_research=False,
                            research_notes=f"Scraped from generic career page on {datetime.now().strftime('%Y-%m-%d')}",
                        )
                        jobs.append(job)

            # Pattern 3: Look for structured job listings (table format, lists, etc.)
            # This is a fallback if headings don't work well
            # Match bullet points or numbered lists that look like job titles
            list_pattern = r"^[\*\-\d\.]+\s+(.+)$"
            list_items = re.findall(list_pattern, content, re.MULTILINE)

            for item in list_items:
                if any(kw in item.lower() for kw in leadership_keywords):
                    # Skip if we already have this title from headings
                    if any(job.title.lower() == item.lower() for job in jobs):
                        continue

                    # Find associated URL
                    item_pos = content.find(item)
                    if item_pos != -1:
                        nearby_content = content[item_pos : item_pos + 500]
                        nearby_urls = re.findall(url_pattern, nearby_content, re.IGNORECASE)
                        job_url = (
                            nearby_urls[0]
                            if nearby_urls
                            else f"https://{company.lower().replace(' ', '')}.com/careers"
                        )

                        job = OpportunityData(
                            source="robotics_deeptech_sheet",
                            source_email="",
                            type="direct_job",
                            company=company,
                            title=item.strip(),
                            location="",
                            link=job_url,
                            description=f"Discovered via Firecrawl scraping of {company} career page",
                            job_type="",
                            needs_research=False,
                            research_notes=f"Scraped from generic career page on {datetime.now().strftime('%Y-%m-%d')}",
                        )
                        jobs.append(job)

            print(f"  âœ“ Extracted {len(jobs)} leadership jobs from {company} markdown")
            return jobs

        except FileNotFoundError:
            print(f"  âœ— Markdown file not found: {markdown_path}")
            return []
        except Exception as e:
            print(f"  âœ— Error processing markdown for {company}: {e}")
            return []

    def run(self, min_score: int = 70, leadership_only: bool = True) -> dict:
        """
        Run weekly scraper job

        Args:
            min_score: Minimum score to store/notify (default: 70 for B+ grade)
            leadership_only: Only scrape leadership roles (default: True)

        Returns:
            Stats dictionary
        """
        print("=" * 80)
        print(f"WEEKLY ROBOTICS JOB SCRAPER - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        print(f"Minimum score: {min_score}")
        print(f"Leadership only: {leadership_only}\n")

        stats = {
            "jobs_scraped": 0,
            "jobs_passed_filter": 0,
            "jobs_above_threshold": 0,
            "jobs_stored": 0,
            "jobs_a_grade": 0,
            "jobs_b_grade": 0,
            "notifications_sent": 0,
            "duplicates_skipped": 0,
            "generic_pages_found": 0,
            "firecrawl_credits_used": 0,
            "firecrawl_failures": 0,
            "firecrawl_successes": 0,
            "firecrawl_disabled": False,
            "github_issues_created": 0,
        }

        # Phase 1: Scrape jobs from Google Sheet
        print("PHASE 1: Scraping robotics/deeptech job board...")
        print("-" * 80)

        # Use Firecrawl fallback method to get both direct jobs and generic pages
        sheet_jobs, generic_pages = self.scraper.scrape_with_firecrawl_fallback()

        # Apply leadership filter if requested
        if leadership_only:
            leadership_keywords = self.scraper.leadership_keywords
            jobs = [
                job
                for job in sheet_jobs
                if any(kw in job.title.lower() for kw in leadership_keywords)
            ]
            print(f"  Filtered for leadership roles: {len(jobs)} of {len(sheet_jobs)} jobs")
        else:
            jobs = sheet_jobs

        stats["jobs_scraped"] = len(jobs)
        stats["generic_pages_found"] = len(generic_pages)
        print(f"  âœ“ Scraped {len(jobs)} direct jobs from sheet")
        print(f"  âœ“ Found {len(generic_pages)} generic career pages from priority companies\n")

        # Step 2: Convert to dicts and filter
        print("Step 2: Filtering for PM/Engineering roles...")
        jobs_as_dicts = []
        for job in jobs:
            jobs_as_dicts.append(
                {
                    "title": job.title,
                    "company": job.company,
                    "location": job.location or "",
                    "link": job.link,
                    "description": job.description or "",
                    "salary": job.salary or "",
                    "job_type": job.job_type or "",
                    "source": "robotics_deeptech_sheet",
                    "source_email": "",
                    "received_at": job.received_at,
                    "keywords_matched": [],
                }
            )

        included, excluded = self.filter.filter_jobs(jobs_as_dicts)
        stats["jobs_passed_filter"] = len(included)
        print(f"  âœ“ Passed filter: {len(included)}")
        print(f"  âœ— Excluded: {len(excluded)}\n")

        # Step 3: Score and filter by threshold
        print(f"Step 3: Scoring jobs (threshold: {min_score}+)...")
        high_scoring_jobs = []

        for job_dict in included:
            score, grade, breakdown = self.scorer.score_job(job_dict)

            if score >= min_score:
                stats["jobs_above_threshold"] += 1

                if grade == "A":
                    stats["jobs_a_grade"] += 1
                elif grade == "B":
                    stats["jobs_b_grade"] += 1

                job_dict["fit_score"] = score
                job_dict["fit_grade"] = grade
                job_dict["score_breakdown"] = json.dumps(breakdown)

                high_scoring_jobs.append(job_dict)

        print(f"  âœ“ High-scoring jobs: {len(high_scoring_jobs)}")
        print(f"    - A grade: {stats['jobs_a_grade']}")
        print(f"    - B grade: {stats['jobs_b_grade']}\n")

        # Step 4: Store and notify
        print("Step 4: Storing and notifying...")

        for job_dict in high_scoring_jobs:
            # Store in database
            job_id = self.database.add_job(job_dict)

            if job_id:
                stats["jobs_stored"] += 1

                # Update score in database
                self.database.update_job_score(
                    job_id,
                    job_dict["fit_score"],
                    job_dict["fit_grade"],
                    job_dict["score_breakdown"],
                )

                print(f"\n  âœ“ New Job: {job_dict['title']}")
                print(f"    Company: {job_dict['company']}")
                print(f"    Score: {job_dict['fit_grade']} ({job_dict['fit_score']}/100)")

                # Send notification for A/B grade jobs
                try:
                    # Add score to notification
                    notification_job = job_dict.copy()
                    notification_job["title"] = (
                        f"[{job_dict['fit_grade']} {job_dict['fit_score']}] {job_dict['title']}"
                    )

                    notification_results = self.notifier.notify_job(notification_job)

                    if notification_results.get("email") or notification_results.get("sms"):
                        stats["notifications_sent"] += 1
                        self.database.mark_notified(job_id)
                        print(
                            f"    âœ“ Notified: SMS={notification_results.get('sms')}, Email={notification_results.get('email')}"
                        )

                except Exception as e:
                    print(f"    âœ— Notification failed: {e}")
            else:
                stats["duplicates_skipped"] += 1
                print(f"\n  - Duplicate: {job_dict['title']} at {job_dict['company']}")

        # Phase 2: Firecrawl Generic Career Pages
        if generic_pages:
            print("\n" + "=" * 80)
            print("PHASE 2: Scraping Generic Career Pages via Firecrawl")
            print("=" * 80)

            # Load config for budget and failure tracking
            config = self.scraper.priority_config
            max_companies = config.get("max_companies_per_run", 10)
            weekly_budget = config.get("credit_budget_weekly", 50)
            failure_threshold = config.get("failure_threshold_percent", 50)
            cache_enabled = config.get("markdown_cache_enabled", True)
            cache_dir = Path(config.get("markdown_cache_dir", "data/firecrawl_cache"))

            print(f"Budget: {weekly_budget} credits/week")
            print(f"Max companies: {max_companies}")
            print(f"Failure threshold: {failure_threshold}%")
            print(f"Markdown cache: {'Enabled' if cache_enabled else 'Disabled'}")
            print(f"Cache directory: {cache_dir}\n")

            # Limit to max companies
            companies_to_scrape = list(generic_pages.items())[:max_companies]
            estimated_credits = len(companies_to_scrape)
            stats["firecrawl_credits_used"] = estimated_credits

            # Check budget
            if estimated_credits > weekly_budget:
                print(
                    f"âš ï¸  WARNING: Estimated {estimated_credits} credits exceeds weekly budget of {weekly_budget}"
                )
                print(f"   Limiting to first {weekly_budget} companies to stay within budget\n")
                companies_to_scrape = companies_to_scrape[:weekly_budget]
                stats["firecrawl_credits_used"] = len(companies_to_scrape)

            print("ðŸ“‹ MANUAL FIRECRAWL COMMANDS (Copy & Execute in Claude Code):")
            print("=" * 80)
            print("Please execute these Firecrawl MCP commands manually:")
            print()

            for i, (company, page_info) in enumerate(companies_to_scrape, 1):
                url = page_info["url"]

                # Generate cache file path
                timestamp = datetime.now().strftime("%Y%m%d")
                cache_file = cache_dir / f"{company.lower().replace(' ', '_')}_{timestamp}.md"

                print(f"[{i}/{len(companies_to_scrape)}] {company}")
                print(f"   URL: {url}")
                print()
                print("   Command:")
                print("   ```python")
                print("   mcp__firecrawl-mcp__firecrawl_scrape(")
                print(f'       url="{url}",')
                print('       formats=["markdown"],')
                print("       onlyMainContent=True")
                print("   )")
                print("   ```")
                print()
                print(f"   Save markdown output to: {cache_file}")
                print()
                print("-" * 80)
                print()

            # Instructions for processing results
            print("\nðŸ“ AFTER RUNNING COMMANDS:")
            print("=" * 80)
            print("1. Save each Firecrawl markdown output to the cache file shown above")
            print("2. Run the markdown processor:")
            print()
            print('   python -c "')
            print("   from src.jobs.weekly_robotics_scraper import WeeklyRoboticsJobChecker")
            print("   checker = WeeklyRoboticsJobChecker()")
            for company, _ in companies_to_scrape:
                timestamp = datetime.now().strftime("%Y%m%d")
                cache_file = cache_dir / f"{company.lower().replace(' ', '_')}_{timestamp}.md"
                print(f'   checker.process_firecrawl_markdown("{cache_file}", "{company}")')
            print('   "')
            print()
            print("3. Results will be automatically scored, stored, and notifications sent")
            print()
            print("âš ï¸  FAILURE TRACKING & AUTO-DISABLE:")
            print(
                f"   If {failure_threshold}%+ companies fail ({int(len(companies_to_scrape) * failure_threshold / 100)}+ of {len(companies_to_scrape)}), Firecrawl will auto-disable"
            )
            print("   Failures trigger automatic GitHub issue creation for investigation")
            print("   Track success/failure counts in scraper logs")
            print("=" * 80)

            # Note: Actual failure tracking would happen during manual execution
            # This is a placeholder to show where the logic would go
            print()
            print("ðŸ“Š MANUAL TRACKING:")
            print("After running all commands, report failures here:")
            print(
                "- Estimate failure rate based on scraping errors (timeout, parsing failures, etc.)"
            )
            print(
                f"- If failure rate > {failure_threshold}%, consider disabling Firecrawl for this week"
            )
            print(
                "- GitHub issues will be created automatically for repeated failures (3+ consecutive)"
            )

        # Summary
        print("\n" + "=" * 80)
        print("WEEKLY SCRAPER COMPLETE")
        print("=" * 80)
        print(f"Jobs scraped: {stats['jobs_scraped']}")
        print(f"Generic career pages: {stats['generic_pages_found']}")
        print(f"Passed filter: {stats['jobs_passed_filter']}")
        print(f"High-scoring (B+): {stats['jobs_above_threshold']}")
        print(f"  - A grade: {stats['jobs_a_grade']}")
        print(f"  - B grade: {stats['jobs_b_grade']}")
        print(f"New jobs stored: {stats['jobs_stored']}")
        print(f"Duplicates skipped: {stats['duplicates_skipped']}")
        print(f"Notifications sent: {stats['notifications_sent']}")
        if stats["firecrawl_credits_used"] > 0:
            print("\nFirecrawl Usage:")
            print(f"  Credits used (estimated): {stats['firecrawl_credits_used']}")
            print(
                f"  Budget remaining: {self.scraper.priority_config.get('credit_budget_weekly', 50) - stats['firecrawl_credits_used']}"
            )
            if stats["firecrawl_failures"] > 0 or stats["firecrawl_successes"] > 0:
                total_attempts = stats["firecrawl_failures"] + stats["firecrawl_successes"]
                failure_rate = (
                    (stats["firecrawl_failures"] / total_attempts * 100)
                    if total_attempts > 0
                    else 0
                )
                print("\nFirecrawl Results:")
                print(f"  Successes: {stats['firecrawl_successes']}")
                print(f"  Failures: {stats['firecrawl_failures']}")
                print(f"  Failure rate: {failure_rate:.1f}%")
                if stats["firecrawl_disabled"]:
                    print("  âš ï¸  AUTO-DISABLED (failure threshold exceeded)")
                if stats["github_issues_created"] > 0:
                    print(f"  GitHub issues created: {stats['github_issues_created']}")

        return stats


def main():
    """CLI entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Weekly robotics/deeptech job scraper")
    parser.add_argument(
        "--min-score", type=int, default=70, help="Minimum score to store/notify (default: 70)"
    )
    parser.add_argument(
        "--all-roles", action="store_true", help="Include IC roles (default: leadership only)"
    )
    parser.add_argument("--dry-run", action="store_true", help="Preview without storing/notifying")

    args = parser.parse_args()

    checker = WeeklyRoboticsJobChecker()

    if args.dry_run:
        print("DRY RUN MODE - No storage or notifications\n")
        # TODO: Implement dry run logic

    stats = checker.run(min_score=args.min_score, leadership_only=not args.all_roles)

    # Output JSON for logging/monitoring
    print("\n" + json.dumps(stats, indent=2))


if __name__ == "__main__":
    main()
